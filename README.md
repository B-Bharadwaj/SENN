# ğŸš€ SENN â€“ Self-Evolving Neural Network **Automatic CNN Architecture Search via Evolutionary Intelligence**

## ğŸ“Œ Overview

**SENN (Self-Evolving Neural Network)** is a research-grade evolutionary framework that **automatically designs, optimizes, and compresses convolutional neural networks (CNNs)** for image classification.

Instead of manually designing neural architectures, SENN treats each CNN as a **genetic individual** and evolves it over multiple generations using principles inspired by **biological evolution**:

- Mutation  
- Crossover  
- Selection  
- Multi-objective optimization  
- Weight inheritance  
- Structured pruning  

The system progressively discovers CNN architectures that achieve **high accuracy with low computational cost**, without human intervention.

---

## âœ¨ Key Features

- ğŸ”¬ Neural Architecture Search (NAS) via evolution  
- ğŸ§¬ CNNs represented as genetic DNA (JSON genotype)  
- âš–ï¸ Multi-objective optimization (accuracy vs efficiency)  
- ğŸ” Weight inheritance for faster convergence  
- âœ‚ï¸ Structured pruning for model compression  
- ğŸ“Š Pareto-optimal selection (NSGA-II)  
- ğŸ“ˆ Rich logging, visualization, and lineage tracking  
- ğŸ–¥ï¸ Optional Streamlit dashboard for live monitoring  

---

## ğŸ§  Core Idea

SENN evolves CNN architectures instead of hand-designing them.

High-level workflow:

```bash
Generate architectures
â†’ Train briefly
â†’ Evaluate
â†’ Select best
â†’ Mutate / Crossover
â†’ Next generation
â†’ Repeat
```

Over generations, the population improves just like biological evolution â€” discovering architectures that balance **performance and efficiency**.

---

## ğŸ“‚ Dataset & Preprocessing

### Dataset
- **Primary:** CIFAR-10  
  - 32Ã—32 RGB images  
  - 10 classes  
- **Optional Extension:** CIFAR-100  

### Preprocessing Pipeline
- Tensor conversion  
- Normalization  
- Optional data augmentation:
  - Random crop
  - Horizontal flip  

### Data Splits
- Training set  
- Validation set  
- Test set  

---

## ğŸ§¬ Architecture Search Space (CNN DNA)

Each CNN is encoded as a **genotype (architecture DNA)** stored in JSON format.

### CNN Constraints

| Component | Options |
|--------|--------|
| Conv layers | 2 â€“ 6 |
| Filters | 16 / 32 / 64 / 128 |
| Kernel sizes | 3Ã—3, 5Ã—5 |
| Activations | ReLU, LeakyReLU |
| Pooling | MaxPool, AvgPool, None |
| Normalization | Optional BatchNorm |
| Regularization | Optional Dropout |
| Head | Global Average Pooling + Dense |
| Model size | Smallâ€“medium CNNs |

The constrained search space ensures **valid, trainable architectures** while allowing rich diversity.

---

## âš™ï¸ Evolution Configuration

| Parameter | Typical Value |
|--------|--------|
| Population size | 8â€“12 |
| Generations | 10â€“20 |
| Survivors | Top-K / Pareto front |
| Training per model | 2â€“3 epochs |
| Total models evaluated | 100+ |

Short training during evolution allows efficient evaluation of many architectures.

---

## ğŸ‹ï¸ Training Strategy

### During Evolution
- Few epochs (2â€“3)
- Goal: estimate architectural potential
- Prevents overfitting and saves compute

### After Evolution
- Best architecture(s) fully trained
- 30â€“50 epochs
- Final evaluation on test set

---

## ğŸ“ Fitness & Evaluation Metrics

SENN uses **multi-objective evaluation**.

### Primary Metrics
- Validation accuracy  
- Validation loss  

### Efficiency Metrics
- Number of parameters  
- FLOPs  
- Inference latency (optional)  

### Fitness Logic
- Early generations: weighted fitness  
- Later generations: Pareto optimization  

This prevents evolution from favoring large, inefficient models.

---

## ğŸ† Selection Mechanisms

### Basic Selection
- Rank by fitness
- Select top-K models

### Advanced Selection
- Pareto front extraction
- **NSGA-II**
  - Non-dominated sorting
  - Crowding distance for diversity

Selected models become **parents** for the next generation.

---

## ğŸ” Mutation Engine (Core Evolution)

Mutation introduces controlled randomness.

### Structural Mutations
- Add / remove convolution layers  
- Increase / decrease filters  
- Change kernel sizes  
- Toggle BatchNorm / Dropout  
- Change pooling strategy  
- Modify dense layer size  
- Adjust learning rate  

All mutations are **constraint-aware**, ensuring valid CNNs.

---

## ğŸ”€ Crossover (Genetic Recombination)

Crossover combines two parent architectures:

- Early convolution blocks from Parent A  
- Later blocks from Parent B  
- Head inherited from one parent  

This encourages exploration beyond local optima.

---

## ğŸ§  Weight Inheritance (Warm Start)

To reduce training cost:

- Layers with identical shapes inherit parent weights  
- New or modified layers are randomly initialized  

This significantly accelerates convergence and mimics biological inheritance.

---

## âœ‚ï¸ Pruning (Model Compression)

SENN integrates pruning for efficiency.

### Pruning Strategies
- Filter/channel reduction via mutation  
- L1-norm based channel pruning  
- Post-training pruning on survivors  

Result: **smaller, faster models with minimal accuracy loss**.

---

## ğŸ”„ Full Evolution Loop

### Initialize population
â†’ Train

â†’ Measure (accuracy, params, FLOPs, latency)

â†’ Select (Pareto / NSGA-II)

â†’ Mutate + Crossover

â†’ Weight inheritance

â†’ Prune

â†’ Validate

â†’ Next generation


Repeated for **N generations**.

---

## ğŸ Final Model Selection

At the end of evolution:

- Extract Pareto-optimal architectures  
- Fully train best candidates  
- Evaluate on test set  

### Example Result
- CIFAR-10 accuracy: ~77â€“80%+  
- Reduced parameters and FLOPs vs baseline CNN  

---

## ğŸ“ Outputs & Artifacts

### Model Files
- `best_model.pth`  
- `best_arch.json`  

### Logs
- `evolution_metrics.csv`  
- `lineage.csv` (parent â†’ child)  
- Mutation history  

### Visualizations
- Accuracy vs generation  
- Pareto fronts  
- Params/FLOPs vs accuracy  
- Confusion matrix  
- Training curves  

---

## ğŸ–¥ï¸ Dashboard & Demo (Optional)

A **Streamlit dashboard** provides:

- Live evolution progress  
- Best architecture summary  
- Pareto front visualization  
- Architecture comparison table  
- Download links for models & DNA  

This transforms SENN from a research prototype into a **usable system**.

---

## ğŸ› ï¸ Phase-Wise Implementation Plan

### Phase 0 â€“ Baseline Evolution (MVP)
- Population
- Mutation
- Selection
- Training loop

### Phase 1 â€“ Architecture DNA
- JSON genotype
- Safe model builder
- Logging

### Phase 2 â€“ Multi-Objective Optimization
- Pareto fronts
- NSGA-II

### Phase 3 â€“ Efficiency Metrics
- Params
- FLOPs
- Latency

### Phase 4 â€“ Crossover
- Genetic recombination

### Phase 5 â€“ Weight Inheritance
- Warm-start children

### Phase 6 â€“ Pruning
- Structured compression

### Phase 7 â€“ Dashboard
- Visualization & interaction

### Phase 8 â€“ Dataset Extension (Optional)
- CIFAR-100
- Custom datasets




